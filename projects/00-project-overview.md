# EVO-X2 ローカルLLM活用プロジェクト全体計画

## プロジェクト概要
AMD Ryzen AI Max+ 395搭載のEVO-X2（96GB統合メモリ）を最大限活用し、完全ローカル環境でエンタープライズグレードのAIシステムを構築する。

## 現在の状況
- ✅ ゲーム起動確認完了
- ✅ LM Studioで基本的な会話実験実施
- ✅ 10-14 tok/secの性能確認
- ✅ Qwen3使用時のVRAM消費率20%確認

## プロジェクト一覧

### 【大項目1】Mastra + k8s マルチLLMオーケストレーション環境
**概要**: 複数のLLMをk8s上で協調動作させる分散AIシステム
**ファイル**: [01-mastra-k8s-orchestration.md](./01-mastra-k8s-orchestration.md)
**優先度**: 高

### 【大項目2】ローカルRAGシステムの構築
**概要**: プライベートデータを安全に活用する検索拡張生成システム
**ファイル**: [02-local-rag-system.md](./02-local-rag-system.md)
**優先度**: 高

### 【大項目3】リアルタイム処理システム
**概要**: 音声アシスタントと開発支援ツールの統合
**ファイル**: [03-realtime-processing.md](./03-realtime-processing.md)
**優先度**: 中

### 【大項目4】大量バッチ処理システム
**概要**: TTFT特性を活かした高速データ処理
**ファイル**: [04-batch-processing.md](./04-batch-processing.md)
**優先度**: 中

### 【大項目5】性能検証とベンチマーク
**概要**: 各モデル・構成の体系的な性能評価
**ファイル**: [05-performance-benchmark.md](./05-performance-benchmark.md)
**優先度**: 中

### 【番外編】ゲーミング性能検証
**概要**: 40CU iGPUでのゲーム動作確認と活用
**ファイル**: [99-gaming-performance.md](./99-gaming-performance.md)
**優先度**: 低

## 技術スタック

### LLMモデル
- Qwen3-32B（日本語・汎用）
- DeepSeek-R1-32B（推論特化）
- Llama 3.3-70B（高性能）
- Sarashina2-70B（日本語特化）

### インフラ・ツール
- Kubernetes（オーケストレーション）
- Ollama/vLLM（モデルサーバー）
- Mastra（エージェントフレームワーク）
- Docker（コンテナ化）

### 開発言語
- TypeScript（Mastra）
- Python（AI処理）
- Go（高性能API）

## スケジュール案

### Phase 1（1-2ヶ月目）
- 大項目5: ベンチマーク実施
- 大項目1: 基盤構築開始

### Phase 2（3-4ヶ月目）
- 大項目1: Mastra統合完了
- 大項目2: RAG基本実装

### Phase 3（5-6ヶ月目）
- 大項目3: リアルタイム処理
- 大項目4: バッチ処理

### 継続的
- 番外編: 息抜きとして随時

## 成功指標

### 技術面
- [ ] 3つ以上のLLM同時稼働
- [ ] 10秒以内のRAG応答
- [ ] 24時間安定稼働

### 実用面
- [ ] 日常業務での活用
- [ ] クラウドAPI費用90%削減
- [ ] 完全オフライン動作

### 体験面
- [ ] 快適な開発体験
- [ ] プライバシーの確保
- [ ] 適度な息抜き環境

## リスクと対策

### リスク
1. メモリ不足
2. 発熱問題
3. 複雑性の増大

### 対策
1. 効率的なモデル量子化
2. 適切な冷却システム
3. 段階的な実装

## 次のステップ
1. このドキュメントのレビュー
2. 優先順位の確定
3. Phase 1の詳細計画策定
4. 環境構築開始

---
*Last Updated: 2025-01-17*