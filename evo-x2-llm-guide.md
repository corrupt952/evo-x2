# EVO-X2 ローカルLLM活用ガイド

## システム概要

**EVO-X2 (AMD Ryzen AI Max+ 395)**
- 96GB統合メモリ（VRAM共有）
- 40CU RDNA 3.5 iGPU
- 世界初70B LLMサポート対応

## ローカルLLMの本質

### 性能特性
- **TTFT (Time to First Token)**: 入力処理速度 - **最大12倍高速**
- **TPS (Tokens per Second)**: 出力生成速度 - **約2倍程度**

### 最適なワークロード
```
✅ 大量入力 → 短い出力  (超高速)
❌ 短い入力 → 長い出力  (普通)
```

## 推奨モデル

### 32B クラス (推奨)
- **Qwen3-32B** - 汎用・日本語・思考モード
- **DeepSeek-R1-32B** - 推論・数学特化

### 70B クラス (上級)
- **Llama 3.3 70B** - GPT-4クラス性能
- **Sarashina2-70B** - 日本語特化

### 複数モデル同時実行
```
Qwen3-32B (30GB) + DeepSeek-R1-32B (25GB) = 55GB
残り: 41GB → さらに追加可能
```

## 実用的活用アイデア

### 1. バッチ処理
- **大量PDFファイル → JSON変換**
- **契約書群 → 項目抽出**
- **議事録 → アクションアイテム**

### 2. 異常検知・監視
- **ログファイル解析 → 異常アラート**
- **売上データ → パターン検出**
- **品質管理 → 不良品検知**

### 3. データ変換・整理
- **文書群 → 構造化データ**
- **アンケート → 感情分析結果**
- **レポート → 要約データベース**

### 4. 自動化ワークフロー
- **日次レポート自動生成**
- **顧客データクレンジング**
- **マーケティングデータ整理**

## 実装のコツ

### ツール選択
- **Ollama**: 簡単導入
- **LM Studio**: GUI操作
- **vLLM**: 高性能API

### 設定推奨
- **量子化**: Q4_K_M (日常用途)
- **Context Length**: 32K
- **VGM設定**: High推奨

### 運用戦略
```
夜間: 大量データ処理 (時間重視しない)
日中: リアルタイム監視 (短時間応答重視)
```

## パフォーマンス特性

### VRAM使用量目安
- **Qwen3-32B**: 約30GB (30%使用)
- **70Bモデル**: 約45GB (47%使用)
- **複数モデル**: 最大85GB程度まで可能

### 実測値
- **入力処理**: Intel比 最大12.2倍高速
- **出力生成**: Intel比 約2.1倍高速
- **メモリ効率**: 96GBの70%余裕あり